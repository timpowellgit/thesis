{
 "metadata": {
  "name": "",
  "signature": "sha256:af75be375c3b83c7f798a71af6ed9bd342187cffffc02ec1fad86d19b0f18cc5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "import networkx as nx\n",
      "import re\n",
      "import collections\n",
      "import operator\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "import random\n",
      "import codecs\n",
      "from codecs import *\n",
      "import os\n",
      "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\n",
      "import numpy as np\n",
      "import re\n",
      "import networkx as nx\n",
      "import math\n",
      "import pickle\n",
      "import operator\n",
      "from sklearn.linear_model import RandomizedLogisticRegression, LogisticRegression\n",
      "from sklearn import grid_search\n",
      "import tempfile\n",
      "from sklearn.externals.joblib import Memory\n",
      "from sklearn.feature_extraction import text \n",
      "from datetime import date\n",
      "import datetime\n",
      "from scipy import spatial\n",
      "from itertools import combinations\n",
      "from sklearn.metrics import roc_curve, auc\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "articles_of_interest = ['10', '2',\n",
      "'5-3',\n",
      "'8-1', \n",
      "'14', \n",
      "'8', \n",
      "'P1-1-1', \n",
      "'5',\n",
      "'3',   \n",
      "'13',\n",
      "'P1-1',\n",
      "'6-1','6']\n",
      "\n",
      "print 'STARTED'\n",
      "\n",
      "graph = nx.read_gexf('../data/echr-judgments-richmeta.gexf')\n",
      "\n",
      "\n",
      "\n",
      "G = nx.read_gexf('../data/echr-judgments-richmeta.gexf')\n",
      "edgelist = G.edges()\n",
      "\n",
      "article = '10'\n",
      "nodes = []\n",
      "for n in G.nodes_iter():\n",
      "    if 'articles' in G.node[n]:\n",
      "        if article in [x for x in re.split(r';|\\+',G.node[n]['articles'])]:\n",
      "            nodes.append(n)\n",
      "subgraph = G.subgraph(nodes)\t\n",
      "eigen = nx.eigenvector_centrality(subgraph)\t\n",
      "h,a = nx.hits(subgraph)\n",
      "indegree = nx.in_degree_centrality(subgraph)\n",
      "edgelist = subgraph.edges()\n",
      "# for pair in edgelist:\n",
      "# \tprint G.node[pair[1]]['date'], '%f' % (eigen[pair[1]]), '%f' %(a[pair[1]]), G.node[pair[0]]['date'], '%f' %(eigen[pair[0]]), '%f' %(a[pair[0]])\n",
      "print 'edgelist loaded', len(edgelist), 'len nodes', len(nodes), 'article: ', article\n",
      "usethislist = os.listdir('../data/echr-copy-1')\n",
      "edgelist2 = []\n",
      "\n",
      "'''\n",
      "make sure to remove the slicing here!!\n",
      "'''\n",
      "for pair in edgelist:\n",
      "    if re.sub('/', '_', pair[0]) in usethislist and re.sub('/', '_', pair[1]) in usethislist:\n",
      "        if 'jud_en' in os.listdir('../data/echr-copy-1/%s' %re.sub('/', '_', pair[0])) and 'jud_en' in os.listdir('../data/echr-copy-1/%s'% re.sub('/', '_', pair[1])):\n",
      "            edgelist2.append(pair)\n",
      "print 'edgelist modified for english only, and in actual data', len(edgelist2)\n",
      "\n",
      "\n",
      "files = [(re.sub('_','/',directory.split('/')[-3])) \n",
      "    for directory, subs, filess in os.walk('../data/echr-copy-1')\n",
      "    for file2 in filess if file2.endswith('citations') and 'jud_en' in directory and re.sub('_','/',directory.split('/')[-3]) in nodes]\n",
      "\n",
      "print 'files opened', len(files)\n",
      "\n",
      "\n",
      "'''\n",
      "remove the slicing here\n",
      "'''\n",
      "percentage = 100\n",
      "sample = len(edgelist2) * int(percentage)/100\n",
      "print('going to load ys, sample is ', sample, ' pairs' )\n",
      "edgelist= edgelist2[0:sample]\n",
      "zeros = []\n",
      "while len(zeros) != len(edgelist):\n",
      "    pair = tuple(random.sample(files, 2))\n",
      "\n",
      "    if pair not in edgelist and pair not in zeros and tuple((pair[1],pair[0])) not in edgelist and tuple((pair[1],pair[0])) not in zeros:\n",
      "        zeros.append(pair)\n",
      "print 'cases sampled'\n",
      "onesandzeros = edgelist+zeros\n",
      "\n",
      "\n",
      "\n",
      "cases = [re.sub('/','_',x) for x in list(set(sum(onesandzeros, ())))]\n",
      "print 'cases listed'\n",
      "files5 = []\n",
      "filenames = []\n",
      "for i,x in enumerate(cases):\n",
      "    dirs = os.listdir('../data/echr-copy-1/%s/jud_en' %(x))\n",
      "    #in case many judgments for that case number use most recent\n",
      "    dir = max([int(y) for y in dirs])\n",
      "    for f in os.listdir('../data/echr-copy-1/%s/jud_en/%s' %(x, dir)):\n",
      "        if f.endswith('citations'):\n",
      "            directory = '../data/echr-copy-1/%s/jud_en/%s' %(x, dir)\n",
      "            files5.append(os.path.join(directory, f))\n",
      "            filenames.append(re.sub('_','/',x))\n",
      "print 'files appended'\n",
      "data = [edgelist,onesandzeros, cases, files5, filenames]\n",
      "# with open('preprocessed.pickle', \"wb\") as f:\n",
      "#     pickle.dump(data, f)\n",
      "\n",
      "vectorizer = CountVectorizer(binary = False, input = 'filename',min_df=4, stop_words =ENGLISH_STOP_WORDS)\n",
      "vocab = vectorizer.fit_transform(files5)\n",
      "vocabtoarray = vocab.toarray()\n",
      "usezero = np.zeros_like(vocabtoarray[0])\n",
      "\n",
      "vectorizerbigram = CountVectorizer(binary = False, input = 'filename', ngram_range=(2, 2),min_df=10, stop_words =ENGLISH_STOP_WORDS)\n",
      "vocabbigram = vectorizerbigram.fit_transform(files5)\n",
      "vocabtoarraybi = vocabbigram.toarray()\n",
      "usezerobi = np.zeros_like(vocabtoarraybi[0])\n",
      "\n",
      "vectorizertrigram = CountVectorizer(binary = False, input = 'filename', ngram_range=(3, 3),min_df=10, stop_words =ENGLISH_STOP_WORDS)\n",
      "vocabtrigram = vectorizertrigram.fit_transform(files5)\n",
      "vocabtoarraytri = vocabtrigram.toarray()\n",
      "usezerotri = np.zeros_like(vocabtoarraytri[0])\n",
      "print 'vectorized'\n",
      "tfidf = TfidfVectorizer(input = 'filename',stop_words =ENGLISH_STOP_WORDS).fit_transform(files5).toarray()\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t    \n",
      "\t\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "STARTED\n",
        "edgelist loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6483 len nodes 878 article:  10\n",
        "edgelist modified for english only, and in actual data"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3622\n",
        "files opened"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 556\n",
        "('going to load ys, sample is ', 3622, ' pairs')\n",
        "cases sampled"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cases listed"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "Xintersection = []\n",
      "Xintersectiontest = []\n",
      "\n",
      "\n",
      "Xintersectionbi = []\n",
      "Xintersectiontestbi = []\n",
      "\n",
      "Xintersectiontri = []\n",
      "Xintersectiontesttri = []\n",
      "\n",
      "\n",
      "days = []\n",
      "daystest = []\n",
      "\n",
      "samecountry = []\n",
      "samecountrytest = []\n",
      "\n",
      "sharedarticles = []\n",
      "sharedarticlestest = []\n",
      "\n",
      "importances = []\n",
      "importancestest = []\n",
      "\n",
      "cosines= []\n",
      "cosinestest = []\n",
      "\n",
      "authorities = []\n",
      "authoritiestest = []\n",
      "\n",
      "eigens = []\n",
      "eigenstest = []\n",
      "\n",
      "indegrees = []\n",
      "indegreestest = []\n",
      "\n",
      "\n",
      "ys = []\n",
      "ystest = []\n",
      "\n",
      "date_format = \"%Y-%m-%d\"\n",
      "\n",
      "for i,pair in enumerate(onesandzeros):\n",
      "    if i%500 ==0:\t\n",
      "        print i,' of ', len(onesandzeros)\n",
      "    indexa = filenames.index(pair[0])\n",
      "    indexb = filenames.index(pair[1])\n",
      "\n",
      "    usezero2 = usezero.copy()\n",
      "    indices= list(set(np.nonzero(vocabtoarray[indexa])[0]).intersection(np.nonzero(vocabtoarray[indexb])[0]))\n",
      "    intersects = np.put(usezero2,indices,1)\n",
      "\n",
      "    usezero2bi = usezerobi.copy()\n",
      "    indicesbi= list(set(np.nonzero(vocabtoarraybi[indexa])[0]).intersection(np.nonzero(vocabtoarraybi[indexb])[0]))\n",
      "    intersectsbi = np.put(usezero2bi,indicesbi,1)\n",
      "\n",
      "    usezero2tri = usezerotri.copy()\n",
      "    indicestri= list(set(np.nonzero(vocabtoarraytri[indexa])[0]).intersection(np.nonzero(vocabtoarraytri[indexb])[0]))\n",
      "    intersectstri = np.put(usezero2tri,indicestri,1)\n",
      "\n",
      "    day = datetime.datetime.strptime('%s' %(G.node[pair[1]]['date']), date_format) - datetime.datetime.strptime('%s' %(G.node[pair[0]]['date']),date_format) \n",
      "\n",
      "    country =1 if G.node[pair[0]]['respondent'] == G.node[pair[1]]['respondent'] else 0\n",
      "\n",
      "    sharedarticle = len(list(set([x for x in re.split(r';|\\+',G.node[pair[0]]['articles'])]).intersection([x for x in re.split(r';|\\+',G.node[pair[1]]['articles'])])))\n",
      "\n",
      "    importanceofsource = G.node[pair[1]]['importance'] if  'importance' in G.node[pair[1]].keys() else '4'\n",
      "\n",
      "    cosine = 1 - spatial.distance.cosine(tfidf[indexa], tfidf[indexb])\n",
      "\n",
      "    if int(G.node[pair[0]]['date'].split('-')[0]) < 2013 or int(G.node[pair[1]]['date'].split('-')[0]) < 2013:\n",
      "        Xintersection.append(usezero2)\n",
      "        Xintersectionbi.append(usezero2bi)\n",
      "        Xintersectiontri.append(usezero2tri)\n",
      "\n",
      "        days.append(day.days)\n",
      "        samecountry.append(country)\n",
      "        sharedarticles.append(sharedarticle)\n",
      "        importances.append(importanceofsource)\n",
      "        cosines.append(cosine)\n",
      "        authorities.append(a[pair[1]])\n",
      "        eigens.append(eigen[pair[1]])\n",
      "        indegrees.append(indegree[pair[1]])\n",
      "\n",
      "        if (i+1)*2 <= len(onesandzeros):\n",
      "            ys.append(1)\n",
      "        else:\n",
      "            ys.append(0)\n",
      "    else:\n",
      "        Xintersectiontest.append(usezero2)\n",
      "        Xintersectiontestbi.append(usezero2bi)\n",
      "        Xintersectiontesttri.append(usezero2tri)\n",
      "\n",
      "        daystest.append(day.days)\n",
      "        samecountrytest.append(country)\n",
      "        sharedarticlestest.append(sharedarticle)\n",
      "        importancestest.append(importanceofsource)\n",
      "        cosinestest.append(cosine)\n",
      "        authoritiestest.append(a[pair[1]])\n",
      "        eigenstest.append(eigen[pair[1]])\n",
      "        indegreestest.append(indegree[pair[1]])\n",
      "        if (i+1)*2 <= len(onesandzeros):\n",
      "            ystest.append(1)\n",
      "        else:\n",
      "            ystest.append(0)\n",
      "        #print('indices put, and appended')\n",
      "print len(Xintersection), len(Xintersectiontest), len(ys), len(ystest)\n",
      "print len(vectorizer.vocabulary_)\n",
      "'''\n",
      "testing\n",
      "'''\n",
      "\n",
      "features = [days,samecountry,sharedarticles,importances,cosines,authorities,eigens,indegrees]\n",
      "features = [daystest,samecountrytest,sharedarticlestest,importancestest,cosinestest,authoritiestest,eigenstest,indegreestest]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'ewr'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}